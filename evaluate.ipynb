{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import random as python_random\n",
    "#import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import auc, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.applications.resnet import ResNet50, preprocess_input\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# pip install image-classifiers==1.0.0b1\n",
    "#from classification_models.tfkeras import Classifiers\n",
    "# More information about this package can be found at https://github.com/qubvel/classification_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.104029</td>\n",
       "      <td>8.283899e-01</td>\n",
       "      <td>0.067581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141493</td>\n",
       "      <td>7.826040e-01</td>\n",
       "      <td>0.075903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184859</td>\n",
       "      <td>5.231151e-03</td>\n",
       "      <td>0.809910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.873188</td>\n",
       "      <td>1.472394e-02</td>\n",
       "      <td>0.112088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180899</td>\n",
       "      <td>1.869610e-03</td>\n",
       "      <td>0.817232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11633</th>\n",
       "      <td>0.005997</td>\n",
       "      <td>3.275743e-06</td>\n",
       "      <td>0.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>0.000180</td>\n",
       "      <td>8.034383e-06</td>\n",
       "      <td>0.999812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>0.003633</td>\n",
       "      <td>8.633947e-07</td>\n",
       "      <td>0.996366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11636</th>\n",
       "      <td>0.020479</td>\n",
       "      <td>1.117117e-04</td>\n",
       "      <td>0.979409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11637</th>\n",
       "      <td>0.231285</td>\n",
       "      <td>2.601236e-03</td>\n",
       "      <td>0.766114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11638 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1         2\n",
       "0      0.104029  8.283899e-01  0.067581\n",
       "1      0.141493  7.826040e-01  0.075903\n",
       "2      0.184859  5.231151e-03  0.809910\n",
       "3      0.873188  1.472394e-02  0.112088\n",
       "4      0.180899  1.869610e-03  0.817232\n",
       "...         ...           ...       ...\n",
       "11633  0.005997  3.275743e-06  0.994000\n",
       "11634  0.000180  8.034383e-06  0.999812\n",
       "11635  0.003633  8.633947e-07  0.996366\n",
       "11636  0.020479  1.117117e-04  0.979409\n",
       "11637  0.231285  2.601236e-03  0.766114\n",
       "\n",
       "[11638 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prediction_df = pd.read_csv('saved_models\\eval_data\\eval_20221117-170236.csv')\n",
    "input_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = test_df\n",
    "\n",
    "true_logits = pd.DataFrame()\n",
    "loss_log = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train    35635\n",
      "val      11715\n",
      "test     11638\n",
      "Name: dataset, dtype: int64\n",
      "Test size:  11715\n",
      "Found 11638 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "df_meta = pd.read_csv('data\\mimic_iv_csv\\df_race_3class_1117.csv')\n",
    "train_size, test_size, val_size = df_meta['dataset'].value_counts()\n",
    "print(df_meta['dataset'].value_counts())\n",
    "print('Test size: ', test_size)\n",
    "\n",
    "# Show the model architecture\n",
    "#print(model.summary())\n",
    "data_dir = 'data/imgs_race'\n",
    "test_dir = f'{data_dir}/test'\n",
    "validate_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_batch_size = 64\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "test_batches = validate_gen.flow_from_directory(\n",
    "                                directory= test_dir,\n",
    "                                classes = None, # means automatically infer the label from subdir\n",
    "                                class_mode = 'categorical', # white, black, asian\n",
    "                                target_size=(HEIGHT, WIDTH),\n",
    "                                shuffle=False,\n",
    "                                # seed=seed, \n",
    "                                batch_size= test_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batches = []\n",
    "i = 0\n",
    "for i in tqdm(range(182)): # 183 is the number of batches\n",
    "    x_batch, y_batch = test_batches[i]\n",
    "    y_batches.append(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11638, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.vstack(y_batches)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/label_race_test.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('data/label_race_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_calc(input_prediction_df, ground_truth = y):\n",
    "    #ground_truth = input_df.race\n",
    "    pathology_array=[\n",
    "        'ASIAN',\n",
    "        'BLACK',\n",
    "        'WHITE'\n",
    "        ]\n",
    "    class_array=[\n",
    "        'ASIAN',\n",
    "        'BLACK',\n",
    "        'WHITE'\n",
    "        ]\n",
    "    i=0\n",
    "    auc_array = []\n",
    "    for i in range(input_prediction_df.shape[0]):\n",
    "        truth = \n",
    "        pred = input_prediction_df[i]\n",
    "        AUC = roc_auc_score(truth, pred)\n",
    "    for pathology in class_array:\n",
    "    \n",
    "        #new_truth = (ground_truth.str.contains(pathology)).apply(int)\n",
    "        new_truth = ground_truth\n",
    "        input_prediction_val = input_prediction_df[i]\n",
    "        val = input_prediction_val\n",
    "        AUC = roc_auc_score(new_truth, val)\n",
    "        true_logits.insert(i, i, new_truth, True)\n",
    "        auc_array.append(AUC)\n",
    "        i += 1\n",
    "        \n",
    "    progress_df = pd.DataFrame({'Study':class_array, 'AUC':auc_array})\n",
    "    print(progress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antho\\Code\\CXR-age\\evaluate.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antho/Code/CXR-age/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_prediction_df\u001b[39m.\u001b[39;49margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\antho\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "input_prediction_df.argmax(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0402870e-01, 8.2838994e-01, 6.7581370e-02],\n",
       "       [1.4149313e-01, 7.8260404e-01, 7.5902930e-02],\n",
       "       [1.8485852e-01, 5.2311514e-03, 8.0991030e-01],\n",
       "       ...,\n",
       "       [3.6328046e-03, 8.6339470e-07, 9.9636626e-01],\n",
       "       [2.0479380e-02, 1.1171170e-04, 9.7940890e-01],\n",
       "       [2.3128456e-01, 2.6012356e-03, 7.6611420e-01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = input_prediction_df.to_numpy()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(pred, axis = 1)\n",
    "y_test = np.argmax(y, axis = 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, printout = False):\n",
    "    # test set\n",
    "    #y_pred = model.predict(X_test) \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    roc = [] #roc_auc_score(y_test, y_pred)\n",
    "    stats_test = [acc, precision, recall, f1, roc]\n",
    "    if printout:\n",
    "        print(f'Test: \\nAccuracy: {acc}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}\\nAUC: {roc}\\n')\n",
    "    \n",
    "    \n",
    "    df_metrics = pd.DataFrame(data= {'Test': stats_test},  \n",
    "                        index=['Accuracy', 'Precision', 'Recall', 'F1-score','AUC score'])\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.81294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.595789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.578234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.577509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC score</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test\n",
       "Accuracy    0.81294\n",
       "Precision  0.595789\n",
       "Recall     0.578234\n",
       "F1-score   0.577509\n",
       "AUC score        []"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Asian       0.13      0.23      0.17       404\n",
      "       Black       0.78      0.59      0.68      2657\n",
      "       White       0.88      0.91      0.89      8577\n",
      "\n",
      "    accuracy                           0.81     11638\n",
      "   macro avg       0.60      0.58      0.58     11638\n",
      "weighted avg       0.83      0.81      0.82     11638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Asian', 'Black', 'White']\n",
    "print (classification_report(test_batches.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Asian       0.13      0.23      0.17       404\n",
      "       Black       0.78      0.59      0.68      2657\n",
      "       White       0.88      0.91      0.89      8577\n",
      "\n",
      "    accuracy                           0.81     11638\n",
      "   macro avg       0.60      0.58      0.58     11638\n",
      "weighted avg       0.83      0.81      0.82     11638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e70f8773578f4b868bb61b386c9d03bafb786dd27355725e49ad26bb1bdc4989"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
